{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and reading the dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle==1.5.6 in c:\\users\\andre\\anaconda3\\lib\\site-packages (1.5.6)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from kaggle==1.5.6) (1.15.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\andre\\anaconda3\\lib\\site-packages (from kaggle==1.5.6) (5.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\andre\\anaconda3\\lib\\site-packages (from kaggle==1.5.6) (4.59.0)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda3\\lib\\site-packages (from kaggle==1.5.6) (2.25.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\andre\\anaconda3\\lib\\site-packages (from kaggle==1.5.6) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\andre\\anaconda3\\lib\\site-packages (from kaggle==1.5.6) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from kaggle==1.5.6) (1.24.3)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from python-slugify->kaggle==1.5.6) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->kaggle==1.5.6) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->kaggle==1.5.6) (4.0.0)\n",
      "Downloading sentiment140.zip to c:\\Users\\andre\\Documents\\Twitter_Sentiment_Analysis\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.9M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/80.9M [00:00<00:43, 1.91MB/s]\n",
      "  2%|▏         | 2.00M/80.9M [00:01<00:39, 2.08MB/s]\n",
      "  4%|▎         | 3.00M/80.9M [00:01<00:36, 2.22MB/s]\n",
      "  5%|▍         | 4.00M/80.9M [00:01<00:36, 2.23MB/s]\n",
      "  6%|▌         | 5.00M/80.9M [00:02<00:34, 2.29MB/s]\n",
      "  7%|▋         | 6.00M/80.9M [00:02<00:34, 2.31MB/s]\n",
      "  9%|▊         | 7.00M/80.9M [00:03<00:33, 2.34MB/s]\n",
      " 10%|▉         | 8.00M/80.9M [00:03<00:32, 2.37MB/s]\n",
      " 11%|█         | 9.00M/80.9M [00:04<00:31, 2.40MB/s]\n",
      " 12%|█▏        | 10.0M/80.9M [00:04<00:31, 2.37MB/s]\n",
      " 14%|█▎        | 11.0M/80.9M [00:05<00:38, 1.93MB/s]\n",
      " 15%|█▍        | 12.0M/80.9M [00:05<00:36, 2.00MB/s]\n",
      " 16%|█▌        | 13.0M/80.9M [00:06<00:33, 2.12MB/s]\n",
      " 17%|█▋        | 14.0M/80.9M [00:06<00:31, 2.21MB/s]\n",
      " 19%|█▊        | 15.0M/80.9M [00:07<00:30, 2.28MB/s]\n",
      " 20%|█▉        | 16.0M/80.9M [00:07<00:29, 2.29MB/s]\n",
      " 21%|██        | 17.0M/80.9M [00:08<00:29, 2.26MB/s]\n",
      " 22%|██▏       | 18.0M/80.9M [00:08<00:29, 2.22MB/s]\n",
      " 23%|██▎       | 19.0M/80.9M [00:08<00:29, 2.23MB/s]\n",
      " 25%|██▍       | 20.0M/80.9M [00:09<00:29, 2.17MB/s]\n",
      " 26%|██▌       | 21.0M/80.9M [00:09<00:29, 2.13MB/s]\n",
      " 27%|██▋       | 22.0M/80.9M [00:10<00:29, 2.09MB/s]\n",
      " 28%|██▊       | 23.0M/80.9M [00:11<00:31, 1.94MB/s]\n",
      " 30%|██▉       | 24.0M/80.9M [00:11<00:29, 2.03MB/s]\n",
      " 31%|███       | 25.0M/80.9M [00:12<00:27, 2.14MB/s]\n",
      " 32%|███▏      | 26.0M/80.9M [00:12<00:26, 2.15MB/s]\n",
      " 33%|███▎      | 27.0M/80.9M [00:13<00:26, 2.13MB/s]\n",
      " 35%|███▍      | 28.0M/80.9M [00:13<00:25, 2.14MB/s]\n",
      " 36%|███▌      | 29.0M/80.9M [00:13<00:24, 2.19MB/s]\n",
      " 37%|███▋      | 30.0M/80.9M [00:14<00:23, 2.23MB/s]\n",
      " 38%|███▊      | 31.0M/80.9M [00:14<00:22, 2.28MB/s]\n",
      " 40%|███▉      | 32.0M/80.9M [00:15<00:22, 2.27MB/s]\n",
      " 41%|████      | 33.0M/80.9M [00:15<00:23, 2.17MB/s]\n",
      " 42%|████▏     | 34.0M/80.9M [00:16<00:23, 2.12MB/s]\n",
      " 43%|████▎     | 35.0M/80.9M [00:17<00:24, 1.95MB/s]\n",
      " 44%|████▍     | 36.0M/80.9M [00:17<00:23, 2.00MB/s]\n",
      " 46%|████▌     | 37.0M/80.9M [00:17<00:22, 2.05MB/s]\n",
      " 47%|████▋     | 38.0M/80.9M [00:18<00:21, 2.11MB/s]\n",
      " 48%|████▊     | 39.0M/80.9M [00:18<00:20, 2.19MB/s]\n",
      " 49%|████▉     | 40.0M/80.9M [00:19<00:19, 2.21MB/s]\n",
      " 51%|█████     | 41.0M/80.9M [00:19<00:18, 2.27MB/s]\n",
      " 52%|█████▏    | 42.0M/80.9M [00:20<00:17, 2.31MB/s]\n",
      " 53%|█████▎    | 43.0M/80.9M [00:20<00:17, 2.32MB/s]\n",
      " 54%|█████▍    | 44.0M/80.9M [00:21<00:16, 2.36MB/s]\n",
      " 56%|█████▌    | 45.0M/80.9M [00:21<00:15, 2.37MB/s]\n",
      " 57%|█████▋    | 46.0M/80.9M [00:22<00:16, 2.19MB/s]\n",
      " 58%|█████▊    | 47.0M/80.9M [00:22<00:15, 2.26MB/s]\n",
      " 59%|█████▉    | 48.0M/80.9M [00:22<00:15, 2.28MB/s]\n",
      " 61%|██████    | 49.0M/80.9M [00:23<00:14, 2.31MB/s]\n",
      " 62%|██████▏   | 50.0M/80.9M [00:23<00:13, 2.32MB/s]\n",
      " 63%|██████▎   | 51.0M/80.9M [00:24<00:13, 2.33MB/s]\n",
      " 64%|██████▍   | 52.0M/80.9M [00:24<00:12, 2.37MB/s]\n",
      " 65%|██████▌   | 53.0M/80.9M [00:25<00:12, 2.38MB/s]\n",
      " 67%|██████▋   | 54.0M/80.9M [00:25<00:11, 2.42MB/s]\n",
      " 68%|██████▊   | 55.0M/80.9M [00:26<00:11, 2.40MB/s]\n",
      " 69%|██████▉   | 56.0M/80.9M [00:26<00:10, 2.40MB/s]\n",
      " 70%|███████   | 57.0M/80.9M [00:26<00:10, 2.42MB/s]\n",
      " 72%|███████▏  | 58.0M/80.9M [00:27<00:10, 2.26MB/s]\n",
      " 73%|███████▎  | 59.0M/80.9M [00:27<00:10, 2.29MB/s]\n",
      " 74%|███████▍  | 60.0M/80.9M [00:28<00:09, 2.37MB/s]\n",
      " 75%|███████▌  | 61.0M/80.9M [00:28<00:08, 2.38MB/s]\n",
      " 77%|███████▋  | 62.0M/80.9M [00:29<00:08, 2.39MB/s]\n",
      " 78%|███████▊  | 63.0M/80.9M [00:29<00:07, 2.39MB/s]\n",
      " 79%|███████▉  | 64.0M/80.9M [00:30<00:07, 2.40MB/s]\n",
      " 80%|████████  | 65.0M/80.9M [00:30<00:06, 2.42MB/s]\n",
      " 82%|████████▏ | 66.0M/80.9M [00:30<00:06, 2.40MB/s]\n",
      " 83%|████████▎ | 67.0M/80.9M [00:31<00:06, 2.39MB/s]\n",
      " 84%|████████▍ | 68.0M/80.9M [00:31<00:05, 2.44MB/s]\n",
      " 85%|████████▌ | 69.0M/80.9M [00:32<00:05, 2.41MB/s]\n",
      " 87%|████████▋ | 70.0M/80.9M [00:32<00:05, 2.20MB/s]\n",
      " 88%|████████▊ | 71.0M/80.9M [00:33<00:04, 2.22MB/s]\n",
      " 89%|████████▉ | 72.0M/80.9M [00:33<00:04, 2.25MB/s]\n",
      " 90%|█████████ | 73.0M/80.9M [00:34<00:03, 2.27MB/s]\n",
      " 91%|█████████▏| 74.0M/80.9M [00:34<00:03, 2.25MB/s]\n",
      " 93%|█████████▎| 75.0M/80.9M [00:35<00:02, 2.25MB/s]\n",
      " 94%|█████████▍| 76.0M/80.9M [00:35<00:02, 2.27MB/s]\n",
      " 95%|█████████▌| 77.0M/80.9M [00:35<00:01, 2.26MB/s]\n",
      " 96%|█████████▋| 78.0M/80.9M [00:36<00:01, 2.25MB/s]\n",
      " 98%|█████████▊| 79.0M/80.9M [00:36<00:00, 2.25MB/s]\n",
      " 99%|█████████▉| 80.0M/80.9M [00:37<00:00, 2.24MB/s]\n",
      "100%|██████████| 80.9M/80.9M [00:37<00:00, 2.09MB/s]\n",
      "100%|██████████| 80.9M/80.9M [00:37<00:00, 2.24MB/s]\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle==1.5.6\n",
    "!kaggle datasets download -d kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"sentiment140.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'Latin-1', names=('target','id','date','flag','user','text'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('training.1600000.processed.noemoticon.csv')\n",
    "os.remove('sentiment140.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    800000\n",
       "4    800000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(message):\n",
    "  return ''.join([char for char in message if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_wo_punctuation'] = df['text'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  return [word for word in simple_preprocess(text) if word not in stop_words and len(word) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_wo_punctuation_and_stopwords'] = df['text_wo_punctuation'].apply(preprocess)\n",
    "df['text_wo_punctuation_and_stopwords_joined'] = df['text_wo_punctuation_and_stopwords'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_wo_punctuation</th>\n",
       "      <th>text_wo_punctuation_and_stopwords</th>\n",
       "      <th>text_wo_punctuation_and_stopwords_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>switchfoot httptwitpiccom2y1zl  Awww thats a b...</td>\n",
       "      <td>[switchfoot, httptwitpiccom, awww, thats, bumm...</td>\n",
       "      <td>switchfoot httptwitpiccom awww thats bummer sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Kenichan I dived many times for the ball Manag...</td>\n",
       "      <td>[kenichan, dived, many, times, ball, managed, ...</td>\n",
       "      <td>kenichan dived many times ball managed save re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>[nationwideclass, behaving, mad, cant, see]</td>\n",
       "      <td>nationwideclass behaving mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                 text_wo_punctuation  \\\n",
       "0  switchfoot httptwitpiccom2y1zl  Awww thats a b...   \n",
       "1  is upset that he cant update his Facebook by t...   \n",
       "2  Kenichan I dived many times for the ball Manag...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  nationwideclass no its not behaving at all im ...   \n",
       "\n",
       "                   text_wo_punctuation_and_stopwords  \\\n",
       "0  [switchfoot, httptwitpiccom, awww, thats, bumm...   \n",
       "1  [upset, cant, update, facebook, texting, might...   \n",
       "2  [kenichan, dived, many, times, ball, managed, ...   \n",
       "3            [whole, body, feels, itchy, like, fire]   \n",
       "4        [nationwideclass, behaving, mad, cant, see]   \n",
       "\n",
       "            text_wo_punctuation_and_stopwords_joined  \n",
       "0  switchfoot httptwitpiccom awww thats bummer sh...  \n",
       "1  upset cant update facebook texting might cry r...  \n",
       "2  kenichan dived many times ball managed save re...  \n",
       "3                   whole body feels itchy like fire  \n",
       "4              nationwideclass behaving mad cant see  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c94be41889154d41bf43aca8d1a8d1cd64b97c119170e03e2ed46ca87183f0c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
